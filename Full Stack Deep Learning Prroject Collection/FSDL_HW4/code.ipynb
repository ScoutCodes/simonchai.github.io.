{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Simon\\anaconda3\\envs\\env2\\lib\\site-packages\\pandas\\core\\computation\\expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "c:\\Users\\Simon\\anaconda3\\envs\\env2\\lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.4' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import torch \n",
    "import torchvision\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from dataclasses import dataclass\n",
    "import time\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "from models import build_model\n",
    "from models.backbone import Backbone, Joiner\n",
    "from models.detr import DETR, PostProcess\n",
    "from models.transformer import Transformer\n",
    "from models.position_encoding import PositionEmbeddingSine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character Recognition Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharacterNetwork(nn.Module):\n",
    "    def __init__(self, hidden_dims=[128,64,32,16]):\n",
    "        super(CharacterNetwork, self).__init__()\n",
    "        self.network_dims = [28 * 28] + hidden_dims + [10]\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        for i in range(len(self.network_dims) - 1):\n",
    "            in_dim = self.network_dims[i]\n",
    "            out_dim = self.network_dims[i + 1]\n",
    "            self.layers.append(nn.Linear(in_dim, out_dim))\n",
    "\n",
    "            # Add activation for hidden layers only\n",
    "            if i < len(self.network_dims) - 2:\n",
    "                self.layers.append(nn.ReLU())\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input x is [B, 1, 28, 28] where B is the batch size\n",
    "        x = x.view(x.size(0), -1)  # Flatten images\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = layer(x)\n",
    "        x = nn.functional.softmax(self.layers[-1](x), dim=1)  # Apply softmax to the final output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = torch.cuda.is_available()\n",
    "model2 = CharacterNetwork(hidden_dims=[128, 64, 32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nms(boxes, scores, threshold):\n",
    "    if len(boxes) == 0:\n",
    "        return []\n",
    "\n",
    "    # Convert boxes to NumPy array\n",
    "    boxes = np.array(boxes)\n",
    "\n",
    "    # Grab the coordinates of the bounding boxes\n",
    "    x1 = boxes[:, 0]\n",
    "    y1 = boxes[:, 1]\n",
    "    x2 = boxes[:, 2]\n",
    "    y2 = boxes[:, 3]\n",
    "\n",
    "    # Compute the area of the bounding boxes\n",
    "    areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
    "\n",
    "    # Sort the bounding boxes by the confidence score\n",
    "    indices = np.argsort(scores)[::-1]\n",
    "\n",
    "    pick = []\n",
    "\n",
    "    while len(indices) > 0:\n",
    "        # Grab the index of the bounding box with the highest score\n",
    "        i = indices[0]\n",
    "        pick.append(i)\n",
    "\n",
    "        # Compute the intersection over union (IoU)\n",
    "        xx1 = np.maximum(x1[i], x1[indices[1:]])\n",
    "        yy1 = np.maximum(y1[i], y1[indices[1:]])\n",
    "        xx2 = np.minimum(x2[i], x2[indices[1:]])\n",
    "        yy2 = np.minimum(y2[i], y2[indices[1:]])\n",
    "\n",
    "        w = np.maximum(0, xx2 - xx1 + 1)\n",
    "        h = np.maximum(0, yy2 - yy1 + 1)\n",
    "\n",
    "        iou = (w * h) / (areas[i] + areas[indices[1:]] - w * h)\n",
    "\n",
    "        # Remove the bounding boxes with IoU greater than the threshold\n",
    "        indices = np.delete(indices, np.concatenate(([0], np.where(iou > threshold)[0] + 1)))\n",
    "\n",
    "    return pick\n",
    "\n",
    "def find_contours(dimensions, img) :\n",
    "    cntrs, _ = cv2.findContours(img.copy(), cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Retrieve potential dimensions\n",
    "    lower_width = dimensions[0]\n",
    "    upper_width = dimensions[1]\n",
    "    lower_height = dimensions[2]\n",
    "    upper_height = dimensions[3]\n",
    "\n",
    "    # Check top 15 largest contours for character\n",
    "    cntrs = sorted(cntrs, key=cv2.contourArea, reverse=True)[:15]\n",
    "\n",
    "\n",
    "    target_contours = []\n",
    "    img_res = []\n",
    "    boxes = []\n",
    "    scores = []\n",
    "    for cntr in cntrs :\n",
    "        # detects contour in binary image and returns the coordinates of rectangle enclosing it\n",
    "        intX, intY, intWidth, intHeight = cv2.boundingRect(cntr)\n",
    "\n",
    "        # checking the dimensions of the contour to filter out the characters by contour's size\n",
    "        if intWidth > lower_width and intWidth < upper_width and intHeight > lower_height and intHeight < upper_height :\n",
    "\n",
    "            char_copy = np.zeros((60,40))\n",
    "            # extracting each character using the enclosing rectangle's coordinates.\n",
    "            char = img[intY:intY+intHeight, intX:intX+intWidth]\n",
    "            char = cv2.resize(char, (20, 40))\n",
    "\n",
    "            char_copy[10:50, 10:30] = char\n",
    "\n",
    "            boxes.append([intX, intY, intX+intWidth, intY+intHeight])\n",
    "            scores.append(intWidth*intHeight)\n",
    "            img_res.append(char_copy) # List that stores the character's binary image (unsorted)\n",
    "\n",
    "    boxes = np.array(boxes)\n",
    "    scores = np.array(scores)\n",
    "    keep = nms(boxes, scores, 0.1)\n",
    "\n",
    "\n",
    "\n",
    "    return [img_res[i] for i in keep], [boxes[i] for i in keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_characters(image, model):\n",
    "    # Check if CUDA is available\n",
    "    cuda = torch.cuda.is_available()\n",
    "    if cuda:\n",
    "        model.cuda()\n",
    "    model.eval()\n",
    "    model.load_state_dict(torch.load(\"CharacterModel.pth\", weights_only=True))\n",
    "\n",
    "    # Store original image dimensions\n",
    "    original_height, original_width = image.shape[:2]\n",
    "\n",
    "    image = img_lp = cv2.resize(image, (333, 75))\n",
    "\n",
    "    # Convert to grayscale and preprocess the license plate image\n",
    "    img_gray_lp = cv2.cvtColor(img_lp, cv2.COLOR_BGR2GRAY)\n",
    "    _, img_binary_lp = cv2.threshold(img_gray_lp, 200, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    img_binary_lp = cv2.erode(img_binary_lp, (3, 3))\n",
    "    img_binary_lp = cv2.dilate(img_binary_lp, (3, 3))\n",
    "\n",
    "    LP_HEIGHT = img_binary_lp.shape[0]\n",
    "    LP_WIDTH = img_binary_lp.shape[1]\n",
    "\n",
    "    # Make borders white (custom preprocessing step)\n",
    "    img_binary_lp[0:3, :] = 255\n",
    "    img_binary_lp[:, 0:3] = 255\n",
    "    img_binary_lp[72:75, :] = 255\n",
    "    img_binary_lp[:, 330:333] = 255\n",
    "\n",
    "    # Estimations of character contours sizes of cropped license plates\n",
    "    dimensions = [0, LP_WIDTH / 2, LP_HEIGHT / 6, 3 * LP_HEIGHT / 3]\n",
    "\n",
    "    # Get contours within cropped license plate\n",
    "    char_list, boxes = find_contours(dimensions, img_binary_lp)\n",
    "    for idx, char in enumerate(char_list):\n",
    "        char = cv2.resize(char, (28, 28))\n",
    "        char = torch.from_numpy(char).reshape(1, 1, 28, 28).float()  # Use reshape instead of resize\n",
    "        if cuda:\n",
    "            char = char.cuda()\n",
    "\n",
    "        outputs = model(char)\n",
    "\n",
    "        pred = torch.argmax(outputs, 1)[0].item()\n",
    "\n",
    "        x1, y1, x2, y2 = boxes[idx]\n",
    "        # Draw bounding box and predicted character on the image\n",
    "        cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 1)\n",
    "        image = cv2.putText(image, f'{pred}', (x1 + 2, y1 + 12), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0, 255, 0), 1, cv2.LINE_AA)\n",
    "\n",
    "    # Resize the processed image back to the original size\n",
    "    resized_image = cv2.resize(image, (original_width, original_height))  # Resize back to original dimensions\n",
    "\n",
    "    # Return the modified image as a NumPy array\n",
    "    return resized_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Simon\\AppData\\Local\\Temp\\ipykernel_7148\\3296271201.py:42: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(ckpt_path, map_location=torch.device('cpu'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Model From checkpoint0199.pth: <All keys matched successfully>\n",
      "Running on local URL:  http://127.0.0.1:7863\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7863/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Simon\\anaconda3\\envs\\env2\\lib\\site-packages\\gradio\\queueing.py\", line 536, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "  File \"c:\\Users\\Simon\\anaconda3\\envs\\env2\\lib\\site-packages\\gradio\\route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"c:\\Users\\Simon\\anaconda3\\envs\\env2\\lib\\site-packages\\gradio\\blocks.py\", line 1935, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"c:\\Users\\Simon\\anaconda3\\envs\\env2\\lib\\site-packages\\gradio\\blocks.py\", line 1520, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "  File \"c:\\Users\\Simon\\anaconda3\\envs\\env2\\lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"c:\\Users\\Simon\\anaconda3\\envs\\env2\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"c:\\Users\\Simon\\anaconda3\\envs\\env2\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 967, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"c:\\Users\\Simon\\anaconda3\\envs\\env2\\lib\\site-packages\\gradio\\utils.py\", line 826, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"C:\\Users\\Simon\\AppData\\Local\\Temp\\ipykernel_7148\\3296271201.py\", line 145, in performDetection\n",
      "    probs, bboxes = detect(Image.fromarray(image), model, transform, threshold=0.9)\n",
      "  File \"c:\\Users\\Simon\\anaconda3\\envs\\env2\\lib\\site-packages\\PIL\\Image.py\", line 2803, in fromarray\n",
      "    arr = obj.__array_interface__\n",
      "AttributeError: 'NoneType' object has no attribute '__array_interface__'\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class Arguments:\n",
    "    device: str = 'cpu'\n",
    "    num_classes: int = 1\n",
    "    backbone: str = \"resnet50\"\n",
    "    hidden_dim: int = 256\n",
    "    dropout: float = 0.1\n",
    "    nheads: int = 8\n",
    "    dim_feedforward: int = 2048\n",
    "    enc_layers: int = 6\n",
    "    dec_layers: int = 6\n",
    "    pre_norm: bool = False\n",
    "    num_queries: int = 10\n",
    "\n",
    "args = Arguments()\n",
    "\n",
    "N_steps = args.hidden_dim // 2\n",
    "\n",
    "position_embedding = PositionEmbeddingSine(N_steps, normalize=True)\n",
    "backbone_ = Backbone(args.backbone, False, False, False)\n",
    "backbone = Joiner(backbone_, position_embedding)\n",
    "backbone.num_channels = backbone_.num_channels\n",
    "transformer = Transformer(\n",
    "        d_model=args.hidden_dim,\n",
    "        dropout=args.dropout,\n",
    "        nhead=args.nheads,\n",
    "        dim_feedforward=args.dim_feedforward,\n",
    "        num_encoder_layers=args.enc_layers,\n",
    "        num_decoder_layers=args.dec_layers,\n",
    "        normalize_before=args.pre_norm,\n",
    "        return_intermediate_dec=True,\n",
    "    )\n",
    "\n",
    "model = DETR(\n",
    "        backbone,\n",
    "        transformer,\n",
    "        num_classes=args.num_classes,\n",
    "        num_queries=args.num_queries,\n",
    ")\n",
    "\n",
    "ckpt_path = \"checkpoint0199.pth\"\n",
    "checkpoint = torch.load(ckpt_path, map_location=torch.device('cpu'))\n",
    "msg = model.load_state_dict(checkpoint['model'])\n",
    "print(f\"Loaded Model From {ckpt_path}: {msg}\")\n",
    "\n",
    "# standard PyTorch mean-std input image normalization\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(800),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# for output bounding box post-processing\n",
    "def box_cxcywh_to_xyxy(x):\n",
    "    x_c, y_c, w, h = x.unbind(1)\n",
    "    b = [(x_c - 0.5 * w), (y_c - 0.5 * h),\n",
    "         (x_c + 0.5 * w), (y_c + 0.5 * h)]\n",
    "    return torch.stack(b, dim=1)\n",
    "\n",
    "def rescale_bboxes(out_bbox, size):\n",
    "    img_w, img_h = size\n",
    "    b = box_cxcywh_to_xyxy(out_bbox)\n",
    "    b = b * torch.tensor([img_w, img_h, img_w, img_h], dtype=torch.float32)\n",
    "    return b\n",
    "\n",
    "def detect(im, model, transform, threshold=0.7):\n",
    "    # mean-std normalize the input image (batch-size: 1)\n",
    "    img = transform(im).unsqueeze(0)\n",
    "\n",
    "    # demo model only support by default images with aspect ratio between 0.5 and 2\n",
    "    # if you want to use images with an aspect ratio outside this range\n",
    "    # rescale your image so that the maximum size is at most 1333 for best results\n",
    "    assert img.shape[-2] <= 1600 and img.shape[-1] <= 1600, 'demo model only supports images up to 1600 pixels on each side'\n",
    "\n",
    "    # propagate through the model\n",
    "    outputs = model(img)\n",
    "\n",
    "    outputs['pred_logits'] = outputs['pred_logits'].cpu()\n",
    "    outputs['pred_boxes'] = outputs['pred_boxes'].cpu()\n",
    "\n",
    "    # keep only predictions with threshold+ confidence\n",
    "    probas = outputs['pred_logits'].softmax(-1)[0, :, :-1]\n",
    "    keep = probas.max(-1).values > threshold\n",
    "\n",
    "    # convert boxes from [0; 1] to image scales\n",
    "    bboxes_scaled = rescale_bboxes(outputs['pred_boxes'][0, keep], im.size)\n",
    "    return probas[keep], bboxes_scaled\n",
    "\n",
    "def detect_characters(image, model):\n",
    "    # Check if CUDA is available\n",
    "    cuda = torch.cuda.is_available()\n",
    "    if cuda:\n",
    "        model.cuda()\n",
    "    model.eval()\n",
    "    model.load_state_dict(torch.load(\"CharacterModel.pth\", weights_only=True))\n",
    "\n",
    "    # Store original image dimensions\n",
    "    original_height, original_width = image.shape[:2]\n",
    "\n",
    "    image = img_lp = cv2.resize(image, (333, 75))\n",
    "\n",
    "    # Convert to grayscale and preprocess the license plate image\n",
    "    img_gray_lp = cv2.cvtColor(img_lp, cv2.COLOR_BGR2GRAY)\n",
    "    _, img_binary_lp = cv2.threshold(img_gray_lp, 200, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    img_binary_lp = cv2.erode(img_binary_lp, (3, 3))\n",
    "    img_binary_lp = cv2.dilate(img_binary_lp, (3, 3))\n",
    "\n",
    "    LP_HEIGHT = img_binary_lp.shape[0]\n",
    "    LP_WIDTH = img_binary_lp.shape[1]\n",
    "\n",
    "    # Make borders white (custom preprocessing step)\n",
    "    img_binary_lp[0:3, :] = 255\n",
    "    img_binary_lp[:, 0:3] = 255\n",
    "    img_binary_lp[72:75, :] = 255\n",
    "    img_binary_lp[:, 330:333] = 255\n",
    "\n",
    "    # Estimations of character contours sizes of cropped license plates\n",
    "    dimensions = [0, LP_WIDTH / 2, LP_HEIGHT / 6, 3 * LP_HEIGHT / 3]\n",
    "\n",
    "    # Get contours within cropped license plate\n",
    "    char_list, boxes = find_contours(dimensions, img_binary_lp)\n",
    "    for idx, char in enumerate(char_list):\n",
    "        char = cv2.resize(char, (28, 28))\n",
    "        char = torch.from_numpy(char).reshape(1, 1, 28, 28).float()  # Use reshape instead of resize\n",
    "        if cuda:\n",
    "            char = char.cuda()\n",
    "\n",
    "        outputs = model(char)\n",
    "\n",
    "        pred = torch.argmax(outputs, 1)[0].item()\n",
    "\n",
    "        x1, y1, x2, y2 = boxes[idx]\n",
    "        # Draw bounding box and predicted character on the image\n",
    "        cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 1)\n",
    "        image = cv2.putText(image, f'{pred}', (x1 + 2, y1 + 12), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0, 255, 0), 1, cv2.LINE_AA)\n",
    "\n",
    "    # Resize the processed image back to the original size\n",
    "    resized_image = cv2.resize(image, (original_width, original_height))  \n",
    "\n",
    "    # Return the modified image as a NumPy array\n",
    "    return resized_image\n",
    "\n",
    "def performDetection(image):\n",
    "    tic = time.time()\n",
    "    probs, bboxes = detect(Image.fromarray(image), model, transform, threshold=0.9)\n",
    "    toc = time.time()\n",
    "\n",
    "    # Process each license plate region\n",
    "    for c, (xmin, ymin, xmax, ymax) in zip(probs, bboxes.tolist()):\n",
    "\n",
    "        # Crop license plate and perform character recognition\n",
    "        cropped_lp = image[int(ymin):int(ymax), int(xmin):int(xmax)]\n",
    "        image = detect_characters(cropped_lp, model2)  # Perform character recognition on the cropped license plate\n",
    "\n",
    "        text = f\"License-Plate: {c.item():0.2f}\"\n",
    "        font_scale = 0.5\n",
    "        txt_size = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, font_scale, 1)[0]\n",
    "        color = (0, 0, 255)\n",
    "        txt_bk_color = [int(v * 0.7) for v in color]\n",
    "        txt_color = (255, 255, 255)\n",
    "        cv2.rectangle(image, (int(xmin), int(ymin)), (int(xmax), int(ymax)), color, 3)\n",
    "        cv2.rectangle(image, (int(xmin), int(ymin)), (int(xmin + txt_size[0] + 1),\n",
    "                                            int(ymin + 1.5 * txt_size[1])), txt_bk_color, -1, )\n",
    "        cv2.putText(image, text, (int(xmin), int(ymin + txt_size[1])),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, font_scale, txt_color, 1, cv2.LINE_AA,)\n",
    "\n",
    "\n",
    "    label = f\"Detecting {len(bboxes)} Bounding Box(es) Take {toc-tic:0.2f} Seconds\"\n",
    "    return image, label\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    css = \"\"\"\n",
    "        h1 {\n",
    "            text-align: center;\n",
    "            display:block;\n",
    "        }\n",
    "        \"\"\"\n",
    "    demo = gr.Blocks(css = css)\n",
    "\n",
    "    with demo:\n",
    "        gr.Markdown(\n",
    "                    \"\"\"\n",
    "                    # Character Recognition\n",
    "                    \"\"\"\n",
    "            )\n",
    "\n",
    "        # Define Layout\n",
    "        with gr.Row():\n",
    "            with gr.Column():\n",
    "                input_image = gr.Image(label = \"Input Image\")\n",
    "                button = gr.Button(\"Detect Characters\")\n",
    "            with gr.Column():\n",
    "                output_image = gr.Image(label = \"Output Image\")\n",
    "                text_label = gr.Label()\n",
    "\n",
    "        # Define Event\n",
    "        button.click(performDetection, inputs = [input_image], outputs=[output_image, text_label])\n",
    "\n",
    "# Launch the interface\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
